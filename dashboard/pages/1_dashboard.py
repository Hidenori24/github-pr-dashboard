# dashboard.py - GitHub PR dashboard (Streamlit)
import re
import time
from datetime import datetime, timedelta, timezone
from typing import Iterable, List, Tuple

import pandas as pd
import plotly.express as px
import streamlit as st
from zoneinfo import ZoneInfo

import action_tracker

import config
from fetcher import run_query
import db_cache  # SQLiteã‚­ãƒ£ãƒƒã‚·ãƒ¥


st.set_page_config(page_title="PRãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", layout="wide", page_icon="")

st.markdown(
    """
    <style>
    h1, h2, h3 { margin-bottom: 0.4rem; }
    section[data-testid=\"stSidebar\"] .stMarkdown { font-size: 0.95rem; }
    div[data-testid=\"stMetric\"] { background: #fafafa; border: 1px solid #eee; border-radius: 12px; padding: 12px; }
    div[data-testid=\"stDataFrame\"] { border: 1px solid #eee; border-radius: 10px; }
    .progress-label { font-weight: 600; }
    .badge {
      display: inline-block; padding: 4px 10px; border-radius: 999px;
      background: #eef2ff; color: #334155; font-size: 0.85rem; margin-right: 6px;
      border: 1px solid #e5e7eb;
    }
    .badge.strong { background: #dcfce7; }
    .small-note { color: #6b7280; font-size: 0.85rem; }
    </style>
    """,
    unsafe_allow_html=True,
)

JST = ZoneInfo("Asia/Tokyo")


def calculate_business_hours(start_dt: datetime, end_dt: datetime) -> float:
    """
    å–¶æ¥­æ—¥ï¼ˆå¹³æ—¥ã®ã¿ï¼‰ã§çµŒéæ™‚é–“ã‚’è¨ˆç®—ã™ã‚‹
    åœŸæ—¥ã‚’é™¤å¤–ã—ã€å–¶æ¥­æ™‚é–“ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    
    Args:
        start_dt: é–‹å§‹æ—¥æ™‚
        end_dt: çµ‚äº†æ—¥æ™‚
    
    Returns:
        å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã®çµŒéæ™‚é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰
    """
    if pd.isna(start_dt) or pd.isna(end_dt):
        return 0.0
    
    # datetimeã«å¤‰æ›
    if isinstance(start_dt, str):
        start_dt = pd.to_datetime(start_dt, format="ISO8601", utc=True)
    if isinstance(end_dt, str):
        end_dt = pd.to_datetime(end_dt, format="ISO8601", utc=True)
    
    # åŒã˜æ—¥ã®å ´åˆã¯å˜ç´”ãªå·®åˆ†
    if start_dt.date() == end_dt.date():
        # åœŸæ—¥ãªã‚‰0ã‚’è¿”ã™
        if start_dt.weekday() >= 5:
            return 0.0
        return (end_dt - start_dt).total_seconds() / 3600
    
    # æ—¥ã‚’ã¾ãŸãå ´åˆã¯æ—¥ã”ã¨ã«è¨ˆç®—
    current = start_dt
    total_hours = 0.0
    
    while current.date() < end_dt.date():
        # å¹³æ—¥ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæœˆæ›œ=0, æ—¥æ›œ=6ï¼‰
        if current.weekday() < 5:
            # ãã®æ—¥ã®æ®‹ã‚Šæ™‚é–“ï¼ˆç¿Œæ—¥0æ™‚ã¾ã§ï¼‰
            next_day = datetime.combine(current.date() + timedelta(days=1), datetime.min.time(), tzinfo=current.tzinfo)
            total_hours += (next_day - current).total_seconds() / 3600
        
        # æ¬¡ã®æ—¥ã¸
        current = datetime.combine(current.date() + timedelta(days=1), datetime.min.time(), tzinfo=current.tzinfo)
    
    # æœ€çµ‚æ—¥ã®æ™‚é–“ã‚’è¿½åŠ 
    if end_dt.weekday() < 5:
        total_hours += (end_dt - current).total_seconds() / 3600
    
    return total_hours


def parse_owner_repo(owner_in: str, repo_in: str) -> Tuple[str, str]:
    """Normalize owner/repo strings even if URLs are provided."""

    def extract_from_url(src: str) -> Tuple[str, str] | None:
        match = re.search(r"github\\.com/([^/\\s]+)/([^/\\s]+)", src or "")
        if match:
            return match.group(1), match.group(2).rstrip("/")
        return None

    parsed = extract_from_url(owner_in) or extract_from_url(repo_in)
    if parsed:
        return parsed
    return (owner_in or "").strip().strip("/"), (repo_in or "").strip().strip("/")


def build_pr_timeline_df(source: pd.DataFrame, compact: bool = False) -> pd.DataFrame:
    """Create a DataFrame tailored for Plotly timeline charts."""

    now_iso = datetime.now(timezone.utc).isoformat()
    df = source.copy()
    df["Start"] = pd.to_datetime(df["createdAt"], format="ISO8601", utc=True)
    df["Finish"] = pd.to_datetime(
        df["mergedAt"].fillna(df["closedAt"]).fillna(now_iso), format="ISO8601", utc=True
    )

    if compact:
        df["Task"] = "#" + df["number"].astype(str)
    else:
        df["Task"] = (
            "#" + df["number"].astype(str)
            + ": "
            + df["title"].fillna("")
            + "  ("
            + df["author"].fillna("")
            + ")"
        )

    df["title_info"] = df["title"].fillna("")
    df["author_info"] = df["author"].fillna("")
    
    # å–¶æ¥­æ—¥ãƒ™ãƒ¼ã‚¹ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
    df["business_hours"] = df.apply(
        lambda row: calculate_business_hours(row["Start"], row["Finish"]),
        axis=1
    )
    df["business_days"] = (df["business_hours"] / 24).round(1)
    
    # æ‹…å½“è€…æƒ…å ±ã‚’è¿½åŠ 
    df["action_owner"] = df.apply(
        lambda row: action_tracker.format_action_for_hover(row.to_dict()), 
        axis=1
    )

    return df[
        [
            "number",
            "Task",
            "Start",
            "Finish",
            "state",
            "age_hours",
            "business_hours",
            "business_days",
            "comments_count",
            "changes_requested",
            "url",
            "title_info",
            "author_info",
            "action_owner",
        ]
    ]


def build_files_table(df_all: pd.DataFrame) -> pd.DataFrame:
    """Explode file paths per PR."""

    pr_cols = [
        "number",
        "title",
        "url",
        "state",
        "author",
        "createdAt",
        "closedAt",
        "mergedAt",
        "age_hours",
        "comments_count",
        "changes_requested",
    ]
    pr_uni = df_all.drop_duplicates("number")[pr_cols + ["files"]].copy()
    pr_uni = pr_uni.explode("files").dropna(subset=["files"])
    if pr_uni.empty:
        return pr_uni
    pr_uni["createdAt_dt"] = pd.to_datetime(pr_uni["createdAt"], format="ISO8601", utc=True)
    return pr_uni


def dir_key(path: str, depth: int) -> str:
    parts = (path or "").split("/")
    if depth <= 0:
        return path or ""
    if len(parts) <= depth:
        return "/".join(parts)
    return "/".join(parts[:depth])


def build_path_tree(paths: Iterable[str]) -> dict:
    tree: dict = {}
    for raw_path in paths:
        path = (raw_path or "").strip("/")
        if not path:
            continue
        parts = path.split("/")
        node = tree
        for part in parts[:-1]:
            node = node.setdefault(part, {})
        node.setdefault("__files__", set()).add(parts[-1])
    return tree


def reset_directory_explorer(key_prefix: str) -> None:
    targets = [
        key
        for key in st.session_state.keys()
        if key.startswith(f"{key_prefix}_dir_") or key == f"{key_prefix}_file"
    ]
    for key in targets:
        del st.session_state[key]


def directory_explorer_v2(
    paths: Iterable[str], key_prefix: str = "explorer"
) -> Tuple[str, str]:
    """
    Windowsé¢¨ã®éšå±¤çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ï¼ˆã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆï¼‰
    Returns: (selected_path, level: 'file' or 'dir')
    """
    tree = build_path_tree(paths)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ç¾åœ¨ã®ãƒ‘ã‚¹ã‚’ç®¡ç†
    if f"{key_prefix}_current_path" not in st.session_state:
        st.session_state[f"{key_prefix}_current_path"] = []
    
    current_path = st.session_state[f"{key_prefix}_current_path"]
    
    # ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã«ç§»å‹•
    node = tree
    for part in current_path:
        node = node.get(part, {})
    
    # ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯å¯èƒ½ï¼‰
    st.markdown("**ğŸ“ ç¾åœ¨åœ°:**")
    breadcrumb_cols = st.columns(len(current_path) + 2)  # root + å„éšå±¤ + ä¸Šã¸ãƒœã‚¿ãƒ³
    
    # rootãƒœã‚¿ãƒ³
    with breadcrumb_cols[0]:
        if st.button("", key=f"{key_prefix}_root", help="ãƒ«ãƒ¼ãƒˆã¸æˆ»ã‚‹"):
            st.session_state[f"{key_prefix}_current_path"] = []
            st.rerun()
    
    # å„éšå±¤ã®ãƒœã‚¿ãƒ³
    for idx, part in enumerate(current_path):
        with breadcrumb_cols[idx + 1]:
            # æœ€å¾Œã®éšå±¤ã¯å¤ªå­—ã§è¡¨ç¤ºï¼ˆç¾åœ¨åœ°ï¼‰
            if idx == len(current_path) - 1:
                st.markdown(f"**/{part}**")
            else:
                if st.button(f"/{part}", key=f"{key_prefix}_bread_{idx}"):
                    st.session_state[f"{key_prefix}_current_path"] = current_path[:idx + 1]
                    st.rerun()
    
    # ä¸Šã¸ãƒœã‚¿ãƒ³
    if current_path:
        with breadcrumb_cols[-1]:
            if st.button("â¬†ï¸ ä¸Šã¸", key=f"{key_prefix}_up"):
                st.session_state[f"{key_prefix}_current_path"] = current_path[:-1]
                st.rerun()
    
    st.markdown("---")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    dirs = sorted([name for name in node.keys() if name != "__files__"])
    files = sorted(node.get("__files__", []))
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒªã‚¹ãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹ãï¼‰
    if dirs:
        st.markdown("### ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€")
        for dir_name in dirs:
            if st.button(f"{dir_name}", key=f"{key_prefix}_dir_{dir_name}", use_container_width=True):
                st.session_state[f"{key_prefix}_current_path"] = current_path + [dir_name]
                st.rerun()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    if files:
        st.markdown("### ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«")
        selected_file = st.radio(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            files,
            key=f"{key_prefix}_file_radio",
            label_visibility="collapsed",
            format_func=lambda x: f"ğŸ“„ {x}"
        )
        if selected_file:
            full_path = "/".join(current_path + [selected_file]) if current_path else selected_file
            return full_path, "file"
    
    if not dirs and not files:
        st.info("ãƒ•ã‚©ãƒ«ãƒ€ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’é¸æŠ
    if current_path:
        full_dir_path = "/".join(current_path)
        return full_dir_path, "dir"
    
    return "", "dir"


def infer_blocker(row: pd.Series, stale_hours: int = 168) -> str | None:
    if row["state"] != "OPEN":
        return None
    if row.get("isDraft"):
        return "Draft"
    if row.get("reviewDecision") == "CHANGES_REQUESTED" or row.get("changes_requested", 0) > 0:
        return "Changes requested"
    checks = (row.get("checks_state") or "").upper()
    if checks in ("FAILURE", "FAILED"):
        return "Checks failing"
    if checks in ("PENDING", "EXPECTED"):
        return "Checks pending"
    if row.get("mergeable") == "CONFLICTING" or row.get("mergeStateStatus") in ("DIRTY", "BEHIND", "BLOCKED"):
        return "Merge conflict"
    if row.get("reviewDecision") == "REVIEW_REQUIRED":
        if row.get("requested_reviewers", 0) > 0:
            return "Waiting for review"
        return "No reviewer"
    age = float(row.get("age_hours") or 0.0)
    if row.get("mergeable") == "MERGEABLE" or row.get("mergeStateStatus") in ("CLEAN", "UNSTABLE", "HAS_HOOKS"):
        return "Ready to merge" if age < stale_hours else "Stale"
    return "Stale" if age >= stale_hours else "Unknown"


def compute_and_cache_stats(owner: str, repo: str, raw_df: pd.DataFrame, filtered_df: pd.DataFrame, files_df_all: pd.DataFrame = None) -> None:
    """
    çµ±è¨ˆæƒ…å ±ã‚’äº‹å‰è¨ˆç®—ã—ã¦DBã«ä¿å­˜ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼å«ã‚€ï¼‰
    """
    stats = {}
    
    # ã‚µãƒãƒªçµ±è¨ˆ
    stats['summary'] = {
        'total_count': len(raw_df),
        'open_count': int((raw_df["state"] == "OPEN").sum()),
        'closed_count': int((raw_df["state"] == "CLOSED").sum()),
        'merged_count': int((raw_df["state"] == "MERGED").sum()),
        'latest_created': raw_df["createdAt_dt"].max().isoformat() if not raw_df.empty else None,
    }
    
    open_only = filtered_df[filtered_df["state"] == "OPEN"]
    if not open_only.empty:
        stats['summary']['median_open_age'] = float(open_only["age_hours"].median())
    else:
        stats['summary']['median_open_age'] = 0.0
    
    # æ»ç•™ãƒã‚±ãƒƒãƒˆé›†è¨ˆ
    if not open_only.empty:
        bucket_counts = (
            open_only.groupby("age_bucket", observed=True)
            .size()
            .reset_index(name="count")
        )
        bucket_counts["age_bucket"] = bucket_counts["age_bucket"].astype(str)
        stats['buckets'] = bucket_counts.to_dict('records')
    
    # DBã«ä¿å­˜
    db_cache.save_aggregated_stats(owner, repo, 'summary', stats)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    if files_df_all is not None and not files_df_all.empty:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ä¸€è¦§ã¨ãƒ„ãƒªãƒ¼æ§‹é€ 
        all_paths = sorted(set(files_df_all["files"].dropna().astype(str)))
        path_tree = build_path_tree(all_paths)
        db_cache.save_file_tree(owner, repo, all_paths, path_tree)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆ
        files_df_copy = files_df_all.copy()
        files_df_copy["dir_key"] = files_df_copy["files"].apply(
            lambda p: "/".join(str(p).split("/")[:-1]) if "/" in str(p) else "(root)"
        )
        
        dir_agg = (
            files_df_copy.groupby("dir_key", observed=True)
            .agg(
                total_prs=("number", lambda s: len(set(s))),
                open_cnt=("state", lambda s: int((s == "OPEN").sum())),
                last_activity=("createdAt_dt", "max"),
            )
            .reset_index()
        )
        db_cache.save_dir_stats(owner, repo, dir_agg)


def load_local_prs(owner: str, repo: str, cutoff_dt) -> tuple:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«DBã‹ã‚‰PRãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆGitHub APIå‘¼ã³å‡ºã—ãªã—ï¼‰
    """
    cached_data = db_cache.load_prs(owner, repo)
    
    if not cached_data:
        return [], "No cache (run: python fetch_data.py)"
    
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
    if cutoff_dt:
        cutoff_iso = cutoff_dt.isoformat()
        cached_data = [
            pr for pr in cached_data 
            if pr.get("createdAt", "") >= cutoff_iso
        ]
    
    return cached_data, "Local cache"


def fetch_and_cache_prs(owner: str, repo: str, cutoff_dt, force_refresh: bool = False):
    """
    PRãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå¼·åˆ¶æ›´æ–°æ™‚ã®ã¿GitHub APIã‚’å‘¼ã³å‡ºã™ï¼‰
    é€šå¸¸ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
    """
    # é€šå¸¸ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
    if not force_refresh:
        return load_local_prs(owner, repo, cutoff_dt)
    
    # å¼·åˆ¶æ›´æ–°ã®å ´åˆã®ã¿GitHub APIã‚’å‘¼ã³å‡ºã™
    etag_info = db_cache.get_etag(owner, repo)
    
    try:
        etag = etag_info["etag"] if etag_info else None
        last_modified = etag_info["last_modified"] if etag_info else None
        
        pr_list, new_etag, new_last_modified, is_modified = run_query(
            owner, repo, 
            cutoff_dt=cutoff_dt,
            etag=etag,
            last_modified=last_modified
        )
        
        # ETagæƒ…å ±ã‚’ä¿å­˜
        if new_etag or new_last_modified:
            db_cache.save_etag(owner, repo, new_etag, new_last_modified)
        
        if is_modified and pr_list:
            # å¤‰æ›´ã‚ã‚Š â†’ DBã«ä¿å­˜
            db_cache.save_prs(owner, repo, pr_list)
            return pr_list, "API (updated)"
        elif not is_modified:
            # å¤‰æ›´ãªã— â†’ DBã‹ã‚‰èª­ã¿è¾¼ã¿
            return load_local_prs(owner, repo, cutoff_dt)
        else:
            # ç©ºã®å ´åˆã‚‚DBã‹ã‚‰
            return load_local_prs(owner, repo, cutoff_dt)
            
    except Exception as e:
        # APIå¤±æ•—æ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        cached_data, source = load_local_prs(owner, repo, cutoff_dt)
        if cached_data:
            return cached_data, f"Cache (API error: {str(e)[:50]})"
        raise


st.title("PR ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

with st.sidebar:
    # ========== ãƒªãƒã‚¸ãƒˆãƒªé¸æŠ ==========
    st.header("ãƒ‡ãƒ¼ã‚¿å–å¾—")
    
    # ãƒªãƒã‚¸ãƒˆãƒªé¸æŠï¼ˆconfig.REPOSITORIESã‹ã‚‰ï¼‰
    if config.REPOSITORIES:
        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¼ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
        default_repo_idx = st.session_state.get('primary_repo_index', 0)
        
        repo_options = [f"{r['name']} ({r['owner']}/{r['repo']})" for r in config.REPOSITORIES]
        selected_repo_idx = st.selectbox(
            "ãƒªãƒã‚¸ãƒˆãƒªé¸æŠ",
            range(len(config.REPOSITORIES)),
            index=default_repo_idx,
            format_func=lambda i: repo_options[i],
            help="config.pyã§è¨­å®šã—ãŸãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰é¸æŠ"
        )
        selected_repo_config = config.REPOSITORIES[selected_repo_idx]
        default_owner = selected_repo_config["owner"]
        default_repo = selected_repo_config["repo"]
        
        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¼ãƒªãƒã‚¸ãƒˆãƒªã®å ´åˆã¯æ˜Ÿå°è¡¨ç¤º
        if selected_repo_idx == st.session_state.get('primary_repo_index', 0):
            st.caption("â­ ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¼ãƒªãƒã‚¸ãƒˆãƒª")
    else:
        default_owner = config.DEFAULT_OWNER
        default_repo = config.DEFAULT_REPO
    
    # æ‰‹å‹•å…¥åŠ›ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰
    with st.expander("æ‰‹å‹•å…¥åŠ›ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰", expanded=False):
        owner_input = st.text_input("Owner ã¾ãŸã¯ URL", value=default_owner, key="manual_owner")
        repo_input = st.text_input("Repo ã¾ãŸã¯ URL", value=default_repo, key="manual_repo")
        use_manual = st.checkbox("æ‰‹å‹•å…¥åŠ›ã‚’ä½¿ç”¨", value=False)
    
    # æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹owner/repo
    if use_manual:
        owner_input_final = owner_input
        repo_input_final = repo_input
    else:
        owner_input_final = default_owner
        repo_input_final = default_repo
    
    st.divider()
    
    st.header("å¯¾è±¡æ¡ä»¶")
    days = st.slider("å¯¾è±¡æœŸé–“ï¼ˆæ—¥ï¼‰", 7, 365, int(config.DEFAULT_DAYS), step=7)
    state_options = ["OPEN", "CLOSED", "MERGED"]
    default_states = [s for s in config.DEFAULT_STATE if s in state_options] or ["OPEN", "MERGED"]
    state_filter = st.multiselect("å¯¾è±¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", state_options, default=default_states)
    stale_hours = st.slider("Stale åˆ¤å®šæ™‚é–“ (h)", 24, 720, 168, step=24)

    st.divider()
    
    st.header("âš™ï¸ è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    show_only_open_groups = st.checkbox(
        "OPENã®ã¿è¡¨ç¤ºï¼ˆæ›¸é¡/ã‚³ãƒ¼ãƒ‰ï¼‰", value=config.DEFAULT_SHOW_ONLY_OPEN_GROUPS
    )
    show_debug = st.checkbox("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º", value=False)
    
    # ========== ãƒ‡ãƒ¼ã‚¿æ›´æ–°ï¼ˆå¿…è¦ãªæ™‚ã ã‘ï¼‰ ==========
    st.divider()
    
    # æ›´æ–°ãƒœã‚¿ãƒ³ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ç®¡ç†ï¼‰
    if "refresh_count" not in st.session_state:
        st.session_state.refresh_count = 0
    
    st.markdown("**ãƒ‡ãƒ¼ã‚¿æ›´æ–°:**")
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        if st.button("GitHubæ›´æ–°", use_container_width=True, type="primary", help="GitHub APIã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"):
            st.session_state.refresh_count += 1
            st.rerun()
    
    with col_btn2:
        if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", use_container_width=True, help="ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"):
            owner, repo = parse_owner_repo(owner_input_final, repo_input_final)
            if owner and repo:
                deleted_pr = db_cache.clear_cache(owner, repo)
                deleted_stats = db_cache.clear_aggregated_stats(owner, repo)
                deleted_files = db_cache.clear_file_caches(owner, repo)
                st.toast(f"PR:{deleted_pr}ä»¶ã€çµ±è¨ˆ:{deleted_stats}ä»¶ã€ãƒ•ã‚¡ã‚¤ãƒ«:{deleted_files}ä»¶å‰Šé™¤")
                st.session_state.refresh_count = 0
                time.sleep(0.5)
                st.rerun()
    
    # ========== ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ï¼ˆå‚è€ƒãƒ»ä¸‹éƒ¨ï¼‰ ==========
    st.divider()
    
    owner_tmp, repo_tmp = parse_owner_repo(owner_input_final, repo_input_final)
    if owner_tmp and repo_tmp:
        cache_info = db_cache.get_cache_info(owner_tmp, repo_tmp)
        etag_info = db_cache.get_etag(owner_tmp, repo_tmp)
        
        with st.expander("ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±", expanded=False):
            if cache_info:
                latest = datetime.fromisoformat(cache_info["latest_fetch"])
                age_hours = (datetime.now(timezone.utc) - latest).total_seconds() / 3600
                
                st.metric("PRæ•°", cache_info['count'])
                st.caption(f"æœ€çµ‚å–å¾—: {age_hours:.1f}æ™‚é–“å‰")
                
                if etag_info:
                    checked = datetime.fromisoformat(etag_info["checked_at"])
                    check_age_min = (datetime.now(timezone.utc) - checked).total_seconds() / 60
                    st.caption(f"æœ€çµ‚ç¢ºèª: {check_age_min:.0f}åˆ†å‰")
                
                # å®šæœŸæ›´æ–°ã®æ¡ˆå†…
                if age_hours > 24:
                    st.warning("ãƒ‡ãƒ¼ã‚¿ãŒ24æ™‚é–“ä»¥ä¸Šå¤ã„ã§ã™")
                    st.caption("ğŸ’¡ `python fetch_data.py` ã§æ›´æ–°")
            else:
                st.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—")
                st.caption("ğŸ’¡ `python fetch_data.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")


owner, repo = parse_owner_repo(owner_input_final, repo_input_final)

if not owner or not repo:
    st.warning("Owner / Repo ã‚’å…¥åŠ›ã—ã¦ã­ã€‚URLã§ã‚‚OKã ã‚ˆï¼")
    st.stop()

progress = st.progress(0)
progress_txt = st.empty()
status_ph = st.empty()


def set_progress(pct: int, message: str = "") -> None:
    pct_val = max(0, min(100, int(pct)))
    progress.progress(pct_val)
    if message:
        progress_txt.markdown(
            f"<span class='progress-label'>{pct_val}% - {message}</span>",
            unsafe_allow_html=True,
        )
    else:
        progress_txt.markdown(
            f"<span class='progress-label'>{pct_val}%</span>",
            unsafe_allow_html=True,
        )


t0 = time.perf_counter()
set_progress(5, "å…¥åŠ›ã‚’ç¢ºèªä¸­")
status_ph.info("PR ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

cutoff_dt = datetime.now(timezone.utc) - timedelta(days=days)

try:
    force_refresh = st.session_state.refresh_count > 0
    data, source = fetch_and_cache_prs(owner, repo, cutoff_dt, force_refresh=force_refresh)
    
    # refresh_countã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ¬¡å›ã¯é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼‰
    if force_refresh and st.session_state.refresh_count > 0:
        st.session_state.refresh_count = 0
    
    if source.startswith("API"):
        status_ph.success(f"GitHub APIã‹ã‚‰{len(data)}ä»¶å–å¾—ã—ã¾ã—ãŸ")
    elif source == "Local cache":
        status_ph.success(f"ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(data)}ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆé«˜é€Ÿè¡¨ç¤ºï¼‰")
    else:
        status_ph.info(f"{source}: {len(data)}ä»¶")
        
except Exception as exc:  # pragma: no cover
    status_ph.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {exc}")
    set_progress(100, "ã‚¨ãƒ©ãƒ¼")
    progress.empty()
    progress_txt.empty()
    st.stop()

set_progress(35, "ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ä¸­")

if not data:
    status_ph.warning("å¯¾è±¡æœŸé–“ã« PR ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚ˆã€‚æœŸé–“ã‚„ãƒªãƒã‚¸ãƒˆãƒªã‚’èª¿æ•´ã—ã¦ã¿ã¦ï¼")
    set_progress(100, "å®Œäº†")
    progress.empty()
    progress_txt.empty()
    st.stop()

raw_df = pd.DataFrame(data)
raw_df["createdAt_dt"] = pd.to_datetime(raw_df["createdAt"], format="ISO8601", utc=True)
raw_df["closedAt_dt"] = pd.to_datetime(raw_df["closedAt"], format="ISO8601", utc=True, errors="coerce")
raw_df["mergedAt_dt"] = pd.to_datetime(raw_df["mergedAt"], format="ISO8601", utc=True, errors="coerce")
raw_df["age_hours"] = pd.to_numeric(raw_df["age_hours"], errors="coerce").fillna(0.0)

bins = [0, 24, 72, 168, 336, 672, 999999]
labels = ["<1d", "1-3d", "3-7d", "7-14d", "14-28d", ">=28d"]
raw_df["age_bucket"] = pd.cut(raw_df["age_hours"], bins=bins, labels=labels, right=False)

if not state_filter:
    status_ph.warning("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒä¸€ã¤ã‚‚é¸ã°ã‚Œã¦ãªã„ã‹ã‚‰ã€å…¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å¯¾è±¡ã«ã™ã‚‹ã­ã€‚")
    state_filter = ["OPEN", "CLOSED", "MERGED"]

filtered_df = raw_df[raw_df["state"].isin(state_filter)].copy()
filtered_df.sort_values("createdAt_dt", ascending=False, inplace=True)
filtered_df.reset_index(drop=True, inplace=True)

set_progress(55, "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ä¸­")

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
files_df_all = build_files_table(filtered_df)

# çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ã—ã¦DBã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆAPIå–å¾—æ™‚ã®ã¿ã€ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å«ã‚€ï¼‰
if source == "API" or source.startswith("API"):
    compute_and_cache_stats(owner, repo, raw_df, filtered_df, files_df_all)

open_only = filtered_df[filtered_df["state"] == "OPEN"].copy()
uniq_all = filtered_df.copy()
uniq = filtered_df.copy()

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµ±è¨ˆã‚’èª­ã¿è¾¼ã¿ï¼ˆ1æ™‚é–“æœ‰åŠ¹ï¼‰
cached_stats = db_cache.load_aggregated_stats(owner, repo, 'summary', max_age_minutes=60)
if cached_stats and 'summary' in cached_stats:
    summary = cached_stats['summary']
    latest_created_iso = summary.get('latest_created')
    if latest_created_iso:
        latest_created = pd.to_datetime(latest_created_iso, utc=True).tz_convert(JST)
    else:
        latest_created = raw_df["createdAt_dt"].max().tz_convert(JST)
    open_count = summary.get('open_count', 0)
    closed_count = summary.get('closed_count', 0)
    merged_count = summary.get('merged_count', 0)
    median_open_age = summary.get('median_open_age', 0.0)
else:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ˆç®—
    latest_created = raw_df["createdAt_dt"].max().tz_convert(JST)
    open_count = int((raw_df["state"] == "OPEN").sum())
    closed_count = int((raw_df["state"] == "CLOSED").sum())
    merged_count = int((raw_df["state"] == "MERGED").sum())
    median_open_age = open_only["age_hours"].median() if not open_only.empty else 0

with st.container():
    now_jst = datetime.now(JST)
    st.markdown(
        f"### {owner}/{repo}"
    )
    st.caption(f"æœ€æ–°PRä½œæˆ: {latest_created.strftime('%Y-%m-%d %H:%M')} JST | "
               f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™: 24æ™‚é–“ | "
               f"è¡¨ç¤ºæ™‚åˆ»: {now_jst.strftime('%Y-%m-%d %H:%M')} JST")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ç·PRä»¶æ•° (ãƒ•ã‚£ãƒ«ã‚¿å¾Œ)", len(uniq))
    col2.metric("OPEN", open_count)
    col3.metric("MERGED", merged_count)
    col4.metric("CLOSED", closed_count)
    st.markdown(
        f"<span class='badge strong'>OPENä¸­å¤®å€¤: {median_open_age:.1f} h</span>"
        f"<span class='badge'>ãƒ‡ãƒ¼ã‚¿æœŸé–“: éå» {days} æ—¥</span>",
        unsafe_allow_html=True,
    )

set_progress(70, "ã‚°ãƒ©ãƒ•æç”»ã®æº–å‚™")

st.markdown("---")

# === ã‚¿ãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³: PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ & æ›¸é¡/ã‚³ãƒ¼ãƒ‰ & ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¾…ã¡ ===
tab1, tab2, tab3 = st.tabs(
    [
        "PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
        "ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´",
        "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¾…ã¡",
    ]
)

with tab1:
    st.markdown('### PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³')
    show_states = st.multiselect(
        "è¡¨ç¤ºã™ã‚‹çŠ¶æ…‹",
        options=["OPEN", "CLOSED", "MERGED"],
        default=config.DEFAULT_GANTT_STATES,
        key="gantt_states",
    )
    initial_top_n = max(10, min(400, int(config.DEFAULT_GANTT_TOP_N)))
    top_n = st.slider(
        "æœ€å¤§è¡¨ç¤ºPRæ•°ï¼ˆæ–°ã—ã„é †ï¼‰",
        10,
        400,
        initial_top_n,
        step=10,
        key="gantt_top_n",
    )
    
    # ã‚½ãƒ¼ãƒˆé †é¸æŠ
    sort_mode = st.radio(
        "ä¸¦ã³é †",
        ["é–‹å§‹ãŒæ–°ã—ã„é †", "é–‹å§‹ãŒå¤ã„é †", "æœŸé–“ãŒé•·ã„é †"],
        index=0,
        horizontal=True,
        key="gantt_sort"
    )
    
    color_mode_options = ["stateï¼ˆçŠ¶æ…‹ï¼‰", "æ»ç•™æ™‚é–“ï¼ˆé€£ç¶šï¼‰"]
    default_color_index = (
        color_mode_options.index(config.DEFAULT_GANTT_COLOR_MODE)
        if config.DEFAULT_GANTT_COLOR_MODE in color_mode_options
        else 0
    )
    color_mode = st.selectbox(
        "è‰²åˆ†ã‘",
        color_mode_options,
        index=default_color_index,
        key="gantt_color",
    )

    src = uniq_all[uniq_all["state"].isin(show_states)].sort_values(
        "createdAt_dt", ascending=False
    )
    src = src.head(top_n)

    if src.empty:
        st.info("è©²å½“ã™ã‚‹PRãŒã‚ã‚Šã¾ã›ã‚“")
    else:
        # compact=True ã§PRç•ªå·ã®ã¿è¡¨ç¤º
        tl_df = build_pr_timeline_df(src, compact=True)

        # PRç•ªå·ã¨URLã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        pr_number_to_url = dict(zip(tl_df["Task"], tl_df["url"]))

        # ã‚½ãƒ¼ãƒˆé †ã‚’é©ç”¨
        if sort_mode == "é–‹å§‹ãŒæ–°ã—ã„é †":
            tl_df = tl_df.sort_values("Start", ascending=True)
        elif sort_mode == "é–‹å§‹ãŒå¤ã„é †":
            tl_df = tl_df.sort_values("Start", ascending=False)
        else:
            tl_df["duration"] = (tl_df["Finish"] - tl_df["Start"]).dt.total_seconds()
            tl_df = tl_df.sort_values("duration", ascending=True)

        task_order = tl_df["Task"].tolist()
        chart_height = max(350, min(1200, len(tl_df) * 18))

        fig_timeline = px.timeline(
            tl_df,
            x_start="Start",
            x_end="Finish",
            y="Task",
            color=("state" if color_mode.startswith("state") else "age_hours"),
            hover_data={
                "number": True,
                "state": True,
                "title_info": True,
                "author_info": True,
                "action_owner": True,
                "age_hours": ":.1f",
                "business_hours": ":.1f",
                "business_days": ":.1f",
                "comments_count": True,
                "changes_requested": True,
                "url": False,
                "Start": False,
                "Finish": False,
            },
            labels={
                "number": "PR#",
                "title_info": "ã‚¿ã‚¤ãƒˆãƒ«",
                "author_info": "ä½œæˆè€…",
                "action_owner": "æ‹…å½“",
                "age_hours": "çµŒéæ™‚é–“(h)",
                "business_hours": "å–¶æ¥­æ™‚é–“(h)",
                "business_days": "å–¶æ¥­æ—¥æ•°",
                "comments_count": "ã‚³ãƒ¡ãƒ³ãƒˆ",
                "changes_requested": "å¤‰æ›´è¦æ±‚",
            },
            category_orders={"Task": task_order},
            height=chart_height,
        )

        fig_timeline.update_traces(
            hovertemplate="<b>%{customdata[2]}</b><br>" +
                         "<b>PR#%{customdata[0]}</b> | %{customdata[1]}<br>" +
                         "ğŸ‘¤ %{customdata[3]}<br>" +
                         "%{customdata[4]}<br>" +
                         "%{customdata[5]:.1f}æ™‚é–“ (%{customdata[7]:.1f} å–¶æ¥­æ—¥)<br>" +
                         "å–¶æ¥­æ™‚é–“: %{customdata[6]:.1f}h<br>" +
                         "%{customdata[8]}ä»¶ | %{customdata[9]}<br>" +
                         "<extra></extra>",
            customdata=tl_df[["number", "state", "title_info", "author_info", 
                             "action_owner", "age_hours", "business_hours", "business_days",
                             "comments_count", "changes_requested"]].values
        )

        fig_timeline.update_layout(
            yaxis={
                'categoryorder': 'array',
                'categoryarray': task_order,
                'automargin': True,
                'type': 'category',
            },
            xaxis_title="æœŸé–“",
            yaxis_title="PR#",
            margin=dict(l=80, r=20, t=40, b=40),
            bargap=0.2,
            bargroupgap=0,
            hovermode='closest',
        )
        fig_timeline.update_yaxes(tickfont=dict(size=10))
        st.plotly_chart(fig_timeline, use_container_width=True, key="timeline_chart")

        st.caption(f"ğŸ’¡ {len(tl_df)}ä»¶è¡¨ç¤ºä¸­ | hover ã§è©³ç´°ç¢ºèª")

with tab2:
    st.markdown("### ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆã‚’èª­ã¿è¾¼ã¿
    cached_tree = db_cache.load_file_tree(owner, repo, max_age_hours=24)
    cached_dir_stats = db_cache.load_dir_stats(owner, repo, max_age_hours=24)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ˆç®—
    if cached_tree is None or cached_dir_stats is None:
        set_progress(72, "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰ä¸­ï¼ˆåˆå›ã®ã¿ï¼‰")
        files_df_all = build_files_table(filtered_df)
        
        if files_df_all.empty:
            st.info("ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            files_df_all = files_df_all.copy()
            files_df_all["files"] = files_df_all["files"].astype(str)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé›†è¨ˆ
            files_df_all["dir_key"] = files_df_all["files"].apply(
                lambda p: "/".join(p.split("/")[:-1]) if "/" in p else "(root)"
            )
            
            dir_agg = (
                files_df_all.groupby("dir_key", observed=True)
                .agg(
                    total_prs=("number", lambda s: len(set(s))),
                    open_cnt=("state", lambda s: int((s == "OPEN").sum())),
                    last_activity=("createdAt_dt", "max"),
                )
                .reset_index()
            )
            
            all_paths = sorted(set(files_df_all["files"].dropna()))
    else:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ
        all_paths, path_tree = cached_tree
        dir_agg = cached_dir_stats
        
        # files_df_allã‚’å¾©å…ƒï¼ˆè©³ç´°è¡¨ç¤ºç”¨ï¼‰
        files_df_all = build_files_table(filtered_df)
        if not files_df_all.empty:
            files_df_all = files_df_all.copy()
            files_df_all["files"] = files_df_all["files"].astype(str)
    
    if files_df_all.empty:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
    else:
        if show_only_open_groups:
            dir_agg = dir_agg[dir_agg["open_cnt"] > 0]
        
        # Recent updateé †ã«ã‚½ãƒ¼ãƒˆ
        dir_agg = dir_agg.sort_values("last_activity", ascending=False)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠç”¨ã®ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ä¸Šéƒ¨ã«é…ç½®ï¼‰
        dir_options = ["ï¼ˆé¸æŠãªã—ï¼‰"] + dir_agg["dir_key"].tolist()
        selected_dir_option = st.selectbox(
            "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠã—ã¦PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤º",
            dir_options,
            index=0,
            key="dir_selector_main"
        )
        
        selected_dir_from_selector = None
        if selected_dir_option != "ï¼ˆé¸æŠãªã—ï¼‰":
            selected_dir_from_selector = selected_dir_option
        
        selected_rows = pd.DataFrame()
        selection_label: str | None = None
        
        # ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰é¸æŠã•ã‚ŒãŸå ´åˆ
        if selected_dir_from_selector:
            if selected_dir_from_selector == "(root)":
                selected_rows = files_df_all[~files_df_all["files"].str.contains("/", na=False)]
            else:
                selected_rows = files_df_all[
                    files_df_all["files"].str.startswith(selected_dir_from_selector + "/")
                ]
            selection_label = selected_dir_from_selector

        # é¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã€PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤º
        if not selected_rows.empty and selection_label:
            display_name = selection_label.split("/")[-1] if "/" in selection_label else selection_label
            if len(display_name) > 50:
                display_name = display_name[:47] + "..."

            st.markdown(f"##### ï¿½ `{display_name}` ã®PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
            pr_numbers = selected_rows["number"].unique().tolist()
            gantt_src = raw_df[raw_df["number"].isin(pr_numbers)].copy()

            if gantt_src.empty:
                st.info("è©²å½“ã™ã‚‹PRãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                # ã‚½ãƒ¼ãƒˆé †é¸æŠ
                file_sort_mode = st.radio(
                    "ä¸¦ã³é †",
                    ["é–‹å§‹ãŒæ–°ã—ã„é †", "é–‹å§‹ãŒå¤ã„é †", "æœŸé–“ãŒé•·ã„é †"],
                    index=0,
                    horizontal=True,
                    key="file_timeline_sort"
                )
                
                gantt_df = build_pr_timeline_df(gantt_src, compact=True)

                # ã‚½ãƒ¼ãƒˆé †ã‚’é©ç”¨
                if file_sort_mode == "é–‹å§‹ãŒæ–°ã—ã„é †":
                    gantt_df = gantt_df.sort_values("Start", ascending=True)
                elif file_sort_mode == "é–‹å§‹ãŒå¤ã„é †":
                    gantt_df = gantt_df.sort_values("Start", ascending=False)
                else:  # "æœŸé–“ãŒé•·ã„é †"
                    gantt_df["duration"] = (gantt_df["Finish"] - gantt_df["Start"]).dt.total_seconds()
                    gantt_df = gantt_df.sort_values("duration", ascending=True)
                
                # Yè»¸ã®é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                task_order = gantt_df["Task"].tolist()

                # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³é«˜ã•ã‚’å‹•çš„èª¿æ•´
                chart_height = max(300, min(800, len(gantt_df) * 18))
                
                fig_file = px.timeline(
                    gantt_df,
                    x_start="Start",
                    x_end="Finish",
                    y="Task",
                    color="state",
                    hover_data={
                        "Task": True,
                        "state": True,
                        "title_info": True,
                        "author_info": True,
                        "action_owner": True,
                        "age_hours": ":.1f",
                        "business_hours": ":.1f",
                        "business_days": ":.1f",
                        "comments_count": True,
                        "changes_requested": True,
                        "url": False,
                        "Start": False,
                        "Finish": False,
                    },
                    labels={
                        "title_info": "ã‚¿ã‚¤ãƒˆãƒ«",
                        "author_info": "ä½œæˆè€…",
                        "action_owner": "æ‹…å½“",
                        "age_hours": "çµŒéæ™‚é–“(h)",
                        "business_hours": "å–¶æ¥­æ™‚é–“(h)",
                        "business_days": "å–¶æ¥­æ—¥æ•°",
                        "comments_count": "ã‚³ãƒ¡ãƒ³ãƒˆæ•°",
                        "changes_requested": "å¤‰æ›´è¦æ±‚",
                    },
                    category_orders={"Task": task_order},
                    height=chart_height,
                )
                
                # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ›ãƒãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
                fig_file.update_traces(
                    hovertemplate="<b>%{customdata[2]}</b><br>" +
                                 "<b>PR#%{customdata[0]}</b> | %{customdata[1]}<br>" +
                                 "ğŸ‘¤ %{customdata[3]}<br>" +
                                 "%{customdata[4]}<br>" +
                                 "%{customdata[5]:.1f}æ™‚é–“ (%{customdata[7]:.1f} å–¶æ¥­æ—¥)<br>" +
                                 "å–¶æ¥­æ™‚é–“: %{customdata[6]:.1f}h<br>" +
                                 "%{customdata[8]}ä»¶ | %{customdata[9]}<br>" +
                                 "<extra></extra>",
                    customdata=gantt_df[["number", "state", "title_info", "author_info", 
                                        "action_owner", "age_hours", "business_hours", "business_days",
                                        "comments_count", "changes_requested"]].values
                )
                
                # Yè»¸ã‚’ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã«ã—ã¦éš™é–“ãªã—
                fig_file.update_layout(
                    yaxis={
                        'categoryorder': 'array',
                        'categoryarray': task_order,
                        'automargin': True,
                        'type': 'category',
                    },
                    xaxis_title="æœŸé–“",
                    yaxis_title="PR#",
                    margin=dict(l=60, r=20, t=20, b=40),
                    bargap=0.2,
                    bargroupgap=0,
                    showlegend=True,
                    hovermode='closest',
                )
                fig_file.update_yaxes(tickfont=dict(size=10))
                st.plotly_chart(fig_file, use_container_width=True, key="file_timeline_chart")

                st.caption(f"ğŸ’¡ {len(gantt_df)}ä»¶ã®PR | hover ã§è©³ç´°ç¢ºèª")
        else:
            st.info("ä¸Šã®ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠã—ã¦PRã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™")
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤ºå¾Œã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆã¨ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ã‚’ä¸‹ã«é…ç½®
        st.markdown("---")
        st.markdown("#### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆ")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆå‚ç…§ç”¨ï¼‰
        dir_display = dir_agg[["dir_key", "open_cnt", "total_prs", "last_activity"]].copy()
        dir_display = dir_display.rename(columns={
            "dir_key": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
            "open_cnt": "OPEN",
            "total_prs": "ç·PR",
            "last_activity": "æœ€çµ‚æ›´æ–°"
        })
        
        st.dataframe(
            dir_display,
            use_container_width=True,
            height=400
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
        with st.expander("ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ï¼ˆéšå±¤é¸æŠï¼‰", expanded=False):
            st.caption("ã‚ˆã‚Šè©³ç´°ã«ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã—ãŸã„å ´åˆã¯ã“ã¡ã‚‰")
            if all_paths:
                # ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ã‚’è¡¨ç¤º
                if cached_tree is not None:
                    _, cached_path_tree = cached_tree
                    directory_explorer_v2(all_paths, key_prefix="file_explorer")
                else:
                    directory_explorer_v2(all_paths, key_prefix="file_explorer")


with tab3:
    st.markdown("### ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¾…ã¡PR")
    
    # OPEN PRã®ã¿å¯¾è±¡
    open_prs = raw_df[raw_df["state"] == "OPEN"].copy()
    
    if open_prs.empty:
        st.info("OPEN PR ãŒã‚ã‚Šã¾ã›ã‚“")
    else:
        # äººã”ã¨ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        user_actions = action_tracker.build_action_summary(open_prs.to_dict('records'))
        
        if not user_actions:
            st.info("ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦ãªPRã¯ã‚ã‚Šã¾ã›ã‚“")
        else:
            st.caption(f"ğŸ“‹ {len(user_actions)}äººã«å¯¾å¿œãŒå¿…è¦ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™")
            
            # äººã”ã¨ã«è¡¨ç¤º
            for user, actions in sorted(user_actions.items(), key=lambda x: len(x[1]), reverse=True):
                action_count = len(actions)
                
                # æ»ç•™ãƒã‚§ãƒƒã‚¯ï¼ˆ168æ™‚é–“ = 7æ—¥ä»¥ä¸Šå¾…ã¡ã®å ´åˆï¼‰
                stale_count = sum(1 for a in actions if a["pr"].get("age_hours", 0) > 168)
                stale_mark = " æ»ç•™ã‚ã‚Š" if stale_count > 0 else ""
                
                with st.expander(f"ğŸ‘¤ **{user}** ({action_count}ä»¶){stale_mark}", expanded=False):
                    for action in sorted(actions, key=lambda x: x["pr"].get("age_hours", 0), reverse=True):
                        pr = action["pr"]
                        action_info = action["action_info"]
                        role = action["role"]
                        
                        age_days = pr.get("age_hours", 0) / 24
                        pr_number = pr.get("number")
                        pr_title = pr.get("title", "")
                        pr_url = pr.get("url", "")
                        author = pr.get("author", "")
                        
                        # çµŒéæ—¥æ•°ã§ãƒãƒ¼ã‚¯
                        age_mark = ""
                        if age_days > 7:
                            age_mark = " ğŸ”´"
                        elif age_days > 3:
                            age_mark = " ğŸŸ¡"
                        
                        role_badge = "âœï¸ ä½œæˆè€…" if role == "author" else "ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼"
                        
                        st.markdown(f"""
**[#{pr_number}]({pr_url})** {pr_title[:60]}{'...' if len(pr_title) > 60 else ''}{age_mark}
- å½¹å‰²: {role_badge} | ç†ç”±: {action_info['reason']} | çµŒé: {age_days:.1f}æ—¥
- ä½œæˆè€…: {author}
                        """)
                        st.divider()


t1 = time.perf_counter()
set_progress(100, "å®Œäº†")
progress.empty()
progress_txt.empty()
status_ph.success("æç”»å®Œäº†ï¼")

now_jst = datetime.now(JST).strftime("%Y-%m-%d %H:%M:%S %Z")
st.caption(f"æœ€çµ‚æ›´æ–°: {now_jst} ï½œ æ‰€è¦æ™‚é–“: {(t1 - t0):.2f} ç§’")

if show_debug:
    with st.expander("æç”»ãƒ­ã‚°ï¼ˆå†…éƒ¨ã‚¹ãƒ†ãƒƒãƒ—ï¼‰"):
        st.write(
            {
                "owner": owner,
                "repo": repo,
                "days": days,
                "state_filter": state_filter,
                "records_after_filter": int(len(uniq)),
            }
        )

